---
id: xleph6dmta3bg5a81gywjp8
title: Basics of Probability
desc: ''
updated: 1650091672580
created: 1650091412529
---

For sample space $S$ and event $E$:

$$
P(S)=1
$$

$$
0\le P(E)\le 1
$$

Mutually exclusive means that the outcome of one ensures that the other will not have the same outcome.

For mutually exclusive events $E$ and $F$ (i.e., $|E\cap F|=0$):

$$
P(E\cup F)=P(E)+P(F)
$$

For independent events $E$ and $F$:

$$
P(E\cap F)=P(E)P(F)
$$

Dependent means that the outcome of one is somehow tied to the outcome of the other one.

Note that it is not possible to have events be mutually exclusive and independent at the same time.

## Conditional Probability

For two events $E$ and $F$, the probability of $E$ after $F$ has happened is:

$$
P(E\,|\,F)=\frac{P(E\cap F)}{P(F)}
$$

The formula comes from looking at size of $E$ but only inside of $F$.

## Bayes Formula

The Conditional Probability formula can be expressed as:

$$
P(F\,|\,E)=\frac{P(E\,|\,F)P(F)}{P(E\,|\,F)P(F)+P(E\,|\,F')P(F')}
$$

Bayes Formula is about updating some probability given new piece of information:

$$
P(F\,|\,E)=P(F)\frac{P(E\,|\,F)}{P(E)}
$$

Here, the prior probability (i.e., probability without this information) which is being updated by a factor representing the information gained. The result is the posterior probability (i.e., probability after knowing this new information).

## Random Variables

For the random variable $X$:

|              | Discrete                        | Continuous                    |
| ------------ | ------------------------------- | ----------------------------- |
| Distribution | Mass                            | Density                       |
| Probability  | $p_X(x)=P(X=x)$                 | $f_X(x)$                      |
| Cumulative   | $F_X(x)=P(X\le x)$              | $F_X(x)=\int_\infty^x f(t)dt$ |
| Set          | $P(X\in T)=\sum_{t\in T}f_X(t)$ | $P(X \in T)=\int_T f(t)dt$    |

## Jointly Distributed Random Variable

Multiple random variables fused into one.

For discrete random variable:
$$
P(X=x,Y=y)=p(x,y)
$$

For continuous random variable:
$$
P((X,Y)\in T)=\int\int_T f(x,y)dx dy
$$

Extract marginal distribution function:

$$
p_X(x)=P(X\le x)=\sum_{y\in Y}p(x,y)
$$
$$
p_Y(y)=P(Y\le y)=\sum_{x\in X}p(x,y)
$$
$$
P(X\le x)=\int_{-\infty}^x\int_{-\infty}^\infty f(t,s)ds dt=\int_{-\infty}^x f_X(t)dt
$$
$$
P(Y\le y)=\int_{-\infty}^y\int_{-\infty}^\infty f(t,s)dt ds=\int_{-\infty}^y f_Y(s)ds
$$

When $X$ and $Y$ are independent, then it is equivalent to:

$$
f(x,y)=f_X(x)f_Y(y)
$$

## Conditional Distributions

The probability density function of $X$ after $Y=y$ is:

$$
f_{X|Y}(x\,|\,y)=\frac{f(x,y)}{f_Y(y)}
$$

## Expectation

The expectation of a random variable $X$ is $E(X)$, which is a value which minimizes the mean squared error: $$E((X-E(X))^2)$$.

For discrete random variable:
$$
E(X)=\sum_x (x)P(X=x)
$$
$$
E(g(X))=\sum_x g(x)P(X=x)
$$

For continuous random variable:
$$
E(X)=\int_{-\infty}^{\infty}(x)f(x)dx
$$
$$
E(g(X))=\int_{-\infty}^{\infty}g(x)f(x)dx
$$

For jointly distributed random variables:
$$
E(g(X,Y))=\int_{-\infty}^\infty g(x,y)f(x,y)dx dy
$$

Properties of expectation:
$$
E(a)=a
$$
$$
E(X+Y)=E(X)+E(Y)
$$
$$
E(aX+b)=aE(X)+b
$$
For independent $X$ and $Y$:
$$
E(XY)=E(X)E(Y)
$$

The $n$<sup>th</sup> moment of $X$ is simply:
$$
E(X^n)
$$

## Variance

$$
Var(X)=E((X-E(X))^2)=E(X^2)-(E(X))^2
$$

Properties of variance:
$$
Var(X)\ge 0
$$
$$
Var(aX+b)=a^2Var(X)
$$
For independent $X$ and $Y$:
$$
Var(aX+bY)=a^2Var(X)+b^2Var(Y)
$$

### Covariance

$$
Cov(X,Y)=E((X-E(X))(Y-E(Y)))=E(XY)-E(X)E(Y)
$$

Note that $Cov(X,X)=Var(X)$.

Properties of covariance:
$$
Cov(X,Y)=Cov(Y,X)
$$
$$
Cov(aX,Y)=a\ Cov(X,Y)
$$
$$
Cov(X+Y,Z)=Cov(X,Z)+Cov(Y,Z)
$$
$$
Var(\sum_{i}X_i)=\sum_{i,j}Cov(X_i,X_j)
$$

If $X$ and $Y$ are independent, then:
$$
Cov(X,Y)=0
$$
Note: the reverse is not necessary true

### Correlation

The correlation coefficient between $X$ and $Y$ is:
$$
Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
$$
Note that $|Corr(X,Y)|\le 1$, and when $Corr(X,Y)=\pm 1$ they are linearly correlated.

## Moment Generating Function

$$
\phi_X(t)=E(e^{tX})
$$
Note: Moment generating function might not always exist

Use for generating $n$<sup>th</sup> moment for $n\ge 1$:
$$
D[\phi_X(0),\,n]=E(X^n)
$$

For independent $X$ and $Y$:
$$
\phi_{X+Y}(t)=\phi_X(t)\phi_Y(t)
$$

Note that moment generating function uniquely determines the distribution.

## Chebyshev's Inequality and Weak Law of Large Numbers

For non-negative random variable $X$ and constant $a>0$, the Markov Inequality states:
$$
P(X\ge a)\le\frac{E(X)}{a}
$$

For random variable with mean $\mu$ and variance $\sigma^2$, and for any constant $k>0$, the Chebyshev's Inequality states:
$$
P(|X-\mu|\ge k)\le\frac{\sigma^2}{k^2}
$$

For $X_i$ be independent identically-distributed random variable with mean $\mu$, and for any constant $\epsilon>0$, the Weak Law of Large Numbers states:
$$
P(|\frac{X_1+...+X_n}{n}-\mu|>\epsilon)\to 0
$$
as $n\to\infty$.
