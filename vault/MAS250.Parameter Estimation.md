---
id: 8v1dvvop0zcjyaax8e317q6
title: Parameter Estimation
desc: ''
updated: 1654841823721
created: 1653824104064
---

## Point Estimates

### Maximum Likelihood Estimator

Let $X$ be a random variables whose probability density function is given by $X(\theta_1,\ldots,\theta_n)\rArr f(x)$. The likelihood of some outcome is given by:

$$
f(x_1,\ldots,x_n\ |\ \theta_1,\ldots,\theta_n)=\prod_{i=1}^n X(\theta_1,\ldots,\theta_n)(x_i)
$$

Then, find the tuple of $\theta$ that maximizes this likelihood function (e.g, via differentiation); this gives us the value of $\theta$. Make sure to do checks on the obtained value to make sure it actually maximizes the likelihood function.

It is also possible to maximize $\log(f(x_1,\ldots,x_n\ |\ \theta_1,\ldots,\theta_n))$ instead if it makes it easier as the maximum occurs with the same parameters.

## Evaluating Point Estimator

Let $X$ be random variable with parameter $\theta$. Supposed $d(X)$ is the obtained estimator.

The bias is:

$$
b_\theta(d)=E\big(d(X)-\theta\big)
$$

If $b_\theta(d)=0$, then this estimator is an unbiased estimator.

The mean square error can be computed using:

$$
r(d_\theta,\ \theta)=E\bigg(\big(d(X)-\theta\big)^2\bigg)
$$

$$
r(d_\theta,\ \theta)=Var(d)+(b_\theta(d))^2
$$

## Interval Estimates

For $X$ being samples from normally distributed random:

$$
\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim Z
$$

So, the $(1-\alpha)\times100\%$ two-sided confidence interval is:

$$
\bar{x}=\pm z_{\alpha/2}\bigg(\frac{\sigma}{\sqrt{n}}\bigg)
$$

Or one-sided confidence intervals:

$$
\bigg(\bar{x}-z_\alpha\frac{\sigma}{n},\infty\bigg)\hspace{4em}\bigg(\infty,\bar{x}+z_\alpha\frac{\sigma}{n}\bigg)
$$

But when the variance is unknown, uses [[T-Distribution|MAS250.Special Continuous Random Variables#t-distribution]] instead:

$$
\frac{\bar{X}-\mu}{S/\sqrt{n}}\sim T_{n-1}
$$

### Prediction Interval

After observing $n$ samples from an unknown normal population, one can prediction the interval of the next sample using:

$$
\Bigg(\bar{x}-t_{\alpha/2,n-1}S\sqrt{n+1\over{n}},\ \bar{x}+t_{\alpha/2,n-1}S\sqrt{n+1\over{n}}\Bigg)
$$

### Variance of Normal Population

From:

$$
\frac{(n-1)S^2}{\sigma^2}\sim\chi_{n-1}^2
$$

So, the confidence interval for $\sigma^2$ is:

$$
\Bigg(\frac{(n-1)s^2}{\chi_{\alpha/2,n-1}^2},\ \frac{(n-1)s^2}{\chi_{1-\alpha/2,n-1}^2}\Bigg)
$$

## Estimates the Differences of Means

We have $X$ and $Y$ from independent normal populations:

$$
\bar{X}-\bar{Y}\sim N\bigg(\mu_x-\mu_y,\ \frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}\bigg)
$$

So, the confidence interval for $\mu_x-\mu_y$ is:

$$
\Bigg(\bar{x}-\bar{y}-z_{\alpha/2}\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}},\ \bar{x}-\bar{y}+z_{\alpha/2}\sqrt{\frac{\sigma_x^2}{n_x}+\frac{\sigma_y^2}{n_y}}\Bigg)
$$

But if $\sigma^2=\sigma_x^2=\sigma_y^2$, then:

$$
\frac{\bar{X}-\bar{Y}-(\mu_x-\mu_y)}{\sqrt{S_p^2\big(\frac{1}{n_x}+\frac{1}{n_y}\big)}}\sim T_{n_x+n_y-2}
$$

So, the confidence interval for $\mu_x-\mu_y$ is:

$$
\Bigg(\bar{x}-\bar{y}-t_{\alpha/2,n_x+n_y-2}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}},\ \bar{x}-\bar{y}+t_{\alpha/2,n_x+n_y-2}s_p\sqrt{\frac{1}{n_x}+\frac{1}{n_y}}\Bigg)
$$

where $S_p^2=\frac{(n_x-1)S_x^2+(n_y-1)S_y^2}{n_x+n_y-2}$.

### Estimates the Probability of [[Bernoulli Distribution|MAS250.Special Discrete Random Variables#bernoulli-distribution]]

From $X\sim B(n,p)$, when $n$ is large, becomes:

$$
\frac{\frac{X}{n}-p}{\sqrt{\frac{(p)(1-p)}{n}}}\sim Z
$$

So, the confidence interval for $p$ is:

$$
\Bigg(\hat{p}-z_{\alpha/2}\sqrt{\frac{(\hat{p})(1-\hat{p})}{n}},\ \hat{p}+z_{\alpha/2}\sqrt{\frac{(\hat{p})(1-\hat{p})}{n}}\Bigg)
$$

### Estimates the Mean of [[Exponential Distribution|MAS250.Special Continuous Random Variables#exponential-distribution]]

Let $X$ be samples from $Exp(\frac{1}{\theta})$, then:

$$
\frac{2}{\theta}X_1\sim Exp(\frac{1}{2})\hspace{4em}\frac{2}{\theta}\sum_{i=1}^n X_i\sim Gamma(n,1/2)=\chi_{2n}^2
$$

So, the confidence interval for $\theta$ is:

$$
(\frac{2\sum_{i=1}^n X_i}{\chi_{\alpha/2,2n}^2}\ ,\frac{2\sum_{i=1}^n X_i}{\chi_{1-\alpha/2,2n}^2})
$$

## Bayes Estimator

We have a prior distribution $p(\theta)$ for $\theta$.

The posterior distribution is:

$$
f(\theta\ |\ x_1,\ldots,x_n)=\frac{f(x_1,\ldots,x_n\ |\ \theta)}{\int f(x_1,\ldots,x_n\ |\ \theta)p(\theta)\ d\theta}p(\theta)
$$

Then, the Bayes Estimator is:

$$
E(\theta\ |\ x_1,\ldots,x_n)=\int \theta f(\theta\ |\ x_1,\ldots,x_n)\ d\theta
$$
